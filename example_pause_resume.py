"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Pause/Resume —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫:
1. –ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ
2. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
3. –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Å –¥—Ä—É–≥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏—Ç–µ—Ä–∞—Ü–∏–π
"""

import torch
from model import CustomTransformerLM
from tokenizer import SimpleTokenizer
from azr_trainer_resume import AZRTrainer
from pathlib import Path
import time
import threading

print("="*70)
print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø PAUSE/RESUME –û–ë–£–ß–ï–ù–ò–Ø")
print("="*70)

# –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
tokenizer = SimpleTokenizer(vocab_size=1000)

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π seed
seed_text = """
The cat sits on the mat. A dog runs in the park.
Birds fly in the sky. The sun shines bright.
Water flows down the river. Trees grow tall.
"""

tokenizer.train([seed_text])
print(f"–°–ª–æ–≤–∞—Ä—å: {len(tokenizer)} —Ç–æ–∫–µ–Ω–æ–≤")

model = CustomTransformerLM(
    vocab_size=len(tokenizer),
    d_model=64,
    num_layers=2,
    num_heads=4,
    d_ff=256,
    max_seq_len=64
)

print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters())}")

# –°–æ–∑–¥–∞—ë–º trainer
trainer = AZRTrainer(model, tokenizer, device='cpu')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
texts = [seed_text[i:i+50] for i in range(0, len(seed_text), 25)]
checkpoint_dir = Path("pause_resume_demo")
checkpoint_dir.mkdir(exist_ok=True)

print("\n" + "="*70)
print("–°–¶–ï–ù–ê–†–ò–ô 1: –ù–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (100 –∏—Ç–µ—Ä–∞—Ü–∏–π)")
print("="*70)

# –û–±—É—á–µ–Ω–∏–µ 1: 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
print("\n–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π...")
history1 = trainer.train_continuous(
    texts=texts,
    max_iterations=100,
    batch_size=4,
    lr=0.001,
    save_every=50,
    checkpoint_dir=checkpoint_dir
)

first_checkpoint = checkpoint_dir / "model_iter_100.pt"
print(f"\n‚úì –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø –∑–∞–≤–µ—Ä—à—ë–Ω. Checkpoint: {first_checkpoint}")
print(f"  –¢–µ–∫—É—â–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è: {trainer.iteration}")

# –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞
print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π:")
model.eval()
tokens = tokenizer.encode("The cat")
if len(tokens) > 0:
    idx = torch.tensor([tokens])
    with torch.no_grad():
        gen1 = model.generate(idx, max_new_tokens=15)
    text1 = tokenizer.decode(gen1[0].tolist())
    print(f"   '{text1}'")

print("\n" + "="*70)
print("–°–¶–ï–ù–ê–†–ò–ô 2: –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –ò–ó–ú–ï–ù–ï–ù–ò–ï–ú –∏—Ç–µ—Ä–∞—Ü–∏–π (100 ‚Üí 250)")
print("="*70)

# –°–æ–∑–¥–∞—ë–º –ù–û–í–´–ô trainer –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
print("\n–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º checkpoint...")
model2 = CustomTransformerLM(
    vocab_size=len(tokenizer),
    d_model=64,
    num_layers=2,
    num_heads=4,
    d_ff=256,
    max_seq_len=64
)

trainer2 = AZRTrainer(model2, tokenizer, device='cpu')

# –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º —Å –ù–û–í–´–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏—Ç–µ—Ä–∞—Ü–∏–π
print(f"\n–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
print(f"  –ë—ã–ª–æ: –¥–æ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π")
print(f"  –°—Ç–∞–ª–æ: –¥–æ 250 –∏—Ç–µ—Ä–∞—Ü–∏–π (–¥–æ–±–∞–≤–∏–ª–∏ +150)")

history2 = trainer2.train_continuous(
    texts=texts,
    max_iterations=250,  # –ò–ó–ú–ï–ù–ò–õ–ò!
    batch_size=4,
    lr=0.001,
    save_every=50,
    checkpoint_dir=checkpoint_dir,
    resume_from=first_checkpoint  # –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º
)

print(f"\n‚úì –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –∑–∞–≤–µ—Ä—à—ë–Ω")
print(f"  –§–∏–Ω–∞–ª—å–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è: {trainer2.iteration}")

# –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞
print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ 250 –∏—Ç–µ—Ä–∞—Ü–∏–π:")
model2.eval()
tokens = tokenizer.encode("The cat")
if len(tokens) > 0:
    idx = torch.tensor([tokens])
    with torch.no_grad():
        gen2 = model2.generate(idx, max_new_tokens=15)
    text2 = tokenizer.decode(gen2[0].tolist())
    print(f"   '{text2}'")

print("\n" + "="*70)
print("–°–¶–ï–ù–ê–†–ò–ô 3: –†—É—á–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ")
print("="*70)

# –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä—É—á–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
model3 = CustomTransformerLM(
    vocab_size=len(tokenizer),
    d_model=64,
    num_layers=2,
    num_heads=4,
    d_ff=256,
    max_seq_len=64
)

trainer3 = AZRTrainer(model3, tokenizer, device='cpu')

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–≤—Ç–æ–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã
def auto_stop():
    time.sleep(3)
    print("\n‚è∏Ô∏è  –ê–≤—Ç–æ–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")
    trainer3.stop_training()

# –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–æ—Å—Ç–∞–Ω–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ
stop_thread = threading.Thread(target=auto_stop)
stop_thread.start()

print("\n–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")
print("(—Å–∏–º—É–ª–∏—Ä—É–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏ 'Stop')")

history3 = trainer3.train_continuous(
    texts=texts,
    max_iterations=1000,  # –ë–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ
    batch_size=4,
    lr=0.001,
    save_every=50,
    checkpoint_dir=checkpoint_dir
)

stop_thread.join()

paused_checkpoint = checkpoint_dir / f"model_paused_{trainer3.iteration}.pt"
print(f"\n‚úì –û–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {trainer3.iteration}")
print(f"  Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {paused_checkpoint}")

# –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º —Å –¥—Ä—É–≥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏—Ç–µ—Ä–∞—Ü–∏–π
print(f"\nüîÑ –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º —Å –Ω–æ–≤–æ–π —Ü–µ–ª—å—é: {trainer3.iteration + 100} –∏—Ç–µ—Ä–∞—Ü–∏–π")

model4 = CustomTransformerLM(
    vocab_size=len(tokenizer),
    d_model=64,
    num_layers=2,
    num_heads=4,
    d_ff=256,
    max_seq_len=64
)

trainer4 = AZRTrainer(model4, tokenizer, device='cpu')

history4 = trainer4.train_continuous(
    texts=texts,
    max_iterations=trainer3.iteration + 100,  # –î–æ–±–∞–≤–ª—è–µ–º 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
    batch_size=4,
    lr=0.001,
    save_every=50,
    checkpoint_dir=checkpoint_dir,
    resume_from=paused_checkpoint
)

print(f"\n‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è: {trainer4.iteration}")

print("\n" + "="*70)
print("–ò–¢–û–ì–ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò")
print("="*70)
print("\n‚úÖ –ß—Ç–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ:")
print("  1. –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç")
print("  2. –ü—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å max_iterations")
print("  3. –ú–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞")
print("  4. Optimizer –∏ scheduler –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è")
print("  5. –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è")

print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ checkpoints:")
for checkpoint in checkpoint_dir.glob("*.pt"):
    size = checkpoint.stat().st_size / 1024
    print(f"  - {checkpoint.name} ({size:.1f} KB)")

print("\nüéØ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ:")
print("  1. –ó–∞–º–µ–Ω–∏—Ç–µ azr_trainer.py –Ω–∞ azr_trainer_resume.py")
print("  2. –î–æ–±–∞–≤—å—Ç–µ –∫–Ω–æ–ø–∫—É 'Stop' –≤ UI")
print("  3. –î–æ–±–∞–≤—å—Ç–µ –ø–æ–ª–µ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è max_iterations –ø—Ä–∏ resume")
print("  4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ resume_from –ø–∞—Ä–∞–º–µ—Ç—Ä")

print("\n" + "="*70)
