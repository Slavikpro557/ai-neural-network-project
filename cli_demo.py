"""
CLI –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è AZR Model Trainer
–°–æ–∑–¥–∞–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
"""

import torch
from pathlib import Path
from model import CustomTransformerLM, count_parameters
from tokenizer import SimpleTokenizer
from azr_trainer import AZRTrainer

def main():
    print("\n" + "="*70)
    print(" üß† AZR Model Trainer - CLI Demo ".center(70, "="))
    print("="*70 + "\n")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    MODEL_NAME = "demo_model"
    BOOK_PATH = Path("books/example_book.txt")
    OUTPUT_DIR = Path("models") / MODEL_NAME
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    VOCAB_SIZE = 5000
    D_MODEL = 256
    NUM_LAYERS = 6
    NUM_HEADS = 8
    MAX_SEQ_LEN = 256
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    MAX_ITERATIONS = 100  # –î–ª—è –¥–µ–º–æ - –º–∞–ª–µ–Ω—å–∫–æ–µ —á–∏—Å–ª–æ
    BATCH_SIZE = 8
    LEARNING_RATE = 3e-4
    
    print("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {D_MODEL}, –°–ª–æ—ë–≤: {NUM_LAYERS}")
    print(f"   –ö–Ω–∏–≥–∞: {BOOK_PATH}")
    print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è: {MAX_ITERATIONS}")
    print()
    
    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
    print("üìö –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–Ω–∏–≥–∏...")
    if not BOOK_PATH.exists():
        print(f"   ‚ùå –§–∞–π–ª {BOOK_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print(f"   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ BOOK_PATH –≤ —Å–∫—Ä–∏–ø—Ç–µ.")
        return
    
    with open(BOOK_PATH, 'r', encoding='utf-8') as f:
        book_text = f.read()
    
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(book_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤: '{book_text[:100]}...'")
    print()
    
    # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("üî§ –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = SimpleTokenizer(vocab_size=VOCAB_SIZE)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    chunks = [book_text[i:i+500] for i in range(0, len(book_text), 250)]
    tokenizer.train(chunks)
    
    print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω. –°–ª–æ–≤–∞—Ä—å: {len(tokenizer)} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer.save(OUTPUT_DIR / "tokenizer.pkl")
    print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_DIR / 'tokenizer.pkl'}")
    print()
    
    # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("üèóÔ∏è  –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = CustomTransformerLM(
        vocab_size=len(tokenizer),
        d_model=D_MODEL,
        num_layers=NUM_LAYERS,
        num_heads=NUM_HEADS,
        d_ff=D_MODEL * 4,
        max_seq_len=MAX_SEQ_LEN,
        dropout=0.1
    )
    
    params = count_parameters(model)
    print(f"   ‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {params:,}")
    print(f"   –ü–∞–º—è—Ç—å: ~{params * 4 / 1024 / 1024:.1f} MB (float32)")
    print()
    
    # –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ —Å AZR
    print("üöÄ –®–∞–≥ 4: –ó–∞–ø—É—Å–∫ AZR –æ–±—É—á–µ–Ω–∏—è...")
    print(f"   –≠—Ç–æ –∑–∞–π–º—ë—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
    print()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print()
    
    trainer = AZRTrainer(model, tokenizer, device=device)
    
    history = trainer.train_continuous(
        texts=chunks,
        max_iterations=MAX_ITERATIONS,
        batch_size=BATCH_SIZE,
        lr=LEARNING_RATE,
        save_every=50,
        checkpoint_dir=OUTPUT_DIR / "checkpoints"
    )
    
    print(f"\n   ‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {history[-1]['loss']:.4f}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π reward: {history[-1]['reward']:.4f}")
    print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    torch.save(model.state_dict(), OUTPUT_DIR / "model_trained.pt")
    print(f"   ‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR / 'model_trained.pt'}")
    print()
    
    # –®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    print("‚ú® –®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
    print()
    
    test_prompts = [
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
        "–ë—É–¥—É—â–µ–µ",
        "–û–±—É—á–µ–Ω–∏–µ",
    ]
    
    for prompt in test_prompts:
        print(f"   –ü—Ä–æ–º–ø—Ç: '{prompt}'")
        
        tokens = tokenizer.encode(prompt)
        idx = torch.tensor([tokens], dtype=torch.long, device=device)
        
        with torch.no_grad():
            generated = model.generate(
                idx, 
                max_new_tokens=50,
                temperature=0.8,
                top_k=40
            )
        
        generated_text = tokenizer.decode(generated[0].cpu().tolist())
        print(f"   ‚Üí {generated_text}")
        print()
    
    print("="*70)
    print(" ‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ".center(70, "="))
    print("="*70)
    print()
    print("üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", OUTPUT_DIR)
    print()
    print("üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: python server.py")
    print("   2. –£–≤–µ–ª–∏—á—å—Ç–µ MAX_ITERATIONS –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("   3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏")
    print("   4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∫–Ω–∏–≥–∏ –∏ —Å—Ç–∏–ª–∏ —Ç–µ–∫—Å—Ç–∞")
    print()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
