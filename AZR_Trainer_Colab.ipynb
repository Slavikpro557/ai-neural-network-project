{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AZR Model Trainer v2 â€” Google Colab\n\n**Neural network trainer with web UI, GPU acceleration, dataset catalog (110 datasets)**\n\n---\n\n### How to use:\n1. **Runtime â†’ Change runtime type â†’ GPU (T4)**\n2. Run all cells in order\n3. Click the **ngrok link** at the end to open the web interface\n\n**GPU works automatically on Colab!** No \"Activate GPU\" needed â€” Tesla T4 + CUDA is detected instantly.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n    print(f'VRAM: {vram / 1024**3:.1f} GB')\nelse:\n    print('WARNING: GPU not found! Go to Runtime -> Change runtime type -> GPU')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone repository & install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Clone repo\nif not os.path.exists('/content/ai-neural-network-project'):\n    !git clone https://github.com/Slavikpro557/ai-neural-network-project.git\nelse:\n    print('Repo already cloned, pulling latest...')\n    !cd /content/ai-neural-network-project && git pull\n\n# Install only missing packages (torch, numpy, psutil already on Colab)\n!pip install -q fastapi uvicorn python-multipart pydantic PyMuPDF pyngrok 2>&1 | tail -5\n\nprint('\\nDone!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Setup ngrok tunnel\n\nngrok creates a public URL to access the server running on Colab.\n\n### How to set up (one time):\n1. Get a free token at https://ngrok.com (sign up â†’ Your Authtoken)\n2. In Colab: click **ðŸ”‘ Secrets** (key icon in left panel)\n3. Add a secret: name = `NGROK_TOKEN`, value = your token\n4. Toggle the switch to enable access for this notebook\n\nYour token is stored **securely in your Google account**, never in the code!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyngrok import ngrok\nimport getpass\n\nNGROK_TOKEN = None\n\n# Method 1: Colab Secrets (recommended)\ntry:\n    from google.colab import userdata\n    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n    print('Token loaded from Colab Secrets!')\nexcept Exception:\n    pass\n\n# Method 2: manual input\nif not NGROK_TOKEN:\n    print('Token not found in Secrets.')\n    print('Paste your ngrok token below (hidden input):')\n    NGROK_TOKEN = getpass.getpass('ngrok token: ')\n\nif NGROK_TOKEN:\n    ngrok.set_auth_token(NGROK_TOKEN)\n    print('ngrok ready!')\nelse:\n    print('WARNING: No token provided. Tunnel may not work.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start AZR Trainer server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport time\nimport torch\nfrom pyngrok import ngrok\n\nos.chdir('/content/ai-neural-network-project')\n\n# Kill any existing server\n!kill $(lsof -t -i:8000) 2>/dev/null; true\ntime.sleep(1)\n\n# Start server in background\nserver_process = subprocess.Popen(\n    ['python', 'server_with_datasets.py'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    cwd='/content/ai-neural-network-project'\n)\n\n# Wait for server to start\nprint('Starting server...')\ntime.sleep(5)\n\n# Check if server started\nif server_process.poll() is None:\n    print('Server is running!')\nelse:\n    print('ERROR: Server failed to start. Check logs below:')\n    print(server_process.stdout.read().decode('utf-8', errors='replace'))\n\n# Open ngrok tunnel\ntry:\n    # Close existing tunnels\n    tunnels = ngrok.get_tunnels()\n    for t in tunnels:\n        ngrok.disconnect(t.public_url)\nexcept:\n    pass\n\npublic_url = ngrok.connect(8000)\n\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n\nprint()\nprint('=' * 60)\nprint(f'  AZR Model Trainer is running!')\nprint(f'  GPU: {gpu_name}')\nprint(f'')\nprint(f'  Open this link:')\nprint(f'  {public_url}')\nprint(f'')\nprint(f'  (keep this cell running!)')\nprint('=' * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Server logs (optional)\n",
    "Run this cell to see server output if something goes wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show last server logs\n",
    "import subprocess\n",
    "result = subprocess.run(['tail', '-30', '/proc/' + str(server_process.pid) + '/fd/1'],\n",
    "                       capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    # Alternative: read from process\n",
    "    print('Server PID:', server_process.pid)\n",
    "    print('Status:', 'Running' if server_process.poll() is None else 'Stopped')\n",
    "else:\n",
    "    print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stop server (when done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to stop everything\n",
    "ngrok.kill()\n",
    "server_process.terminate()\n",
    "print('Server stopped. Tunnel closed.')"
   ]
  }
 ]
}