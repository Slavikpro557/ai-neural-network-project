# üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AZR Model Trainer

## –û–±–∑–æ—Ä

AZR Model Trainer - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏—Ö—Å—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–µ—Ç–æ–¥–µ **Absolute Zero Reasoner**. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è: –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —É–ª—É—á—à–∞—Ç—å —Å–µ–±—è —á–µ—Ä–µ–∑ **self-play** –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–º –≤–Ω–µ—à–Ω–µ–º –Ω–∞–¥–∑–æ—Ä–µ.

---

## üìä –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

### 1. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (`model.py`)

#### CustomTransformerLM
–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞:

```
–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    ‚Üì
Token Embedding + Position Embedding
    ‚Üì
Transformer Blocks (N —Å–ª–æ—ë–≤)
    ‚îú‚îÄ Multi-Head Self-Attention
    ‚îú‚îÄ LayerNorm
    ‚îú‚îÄ Feed-Forward Network
    ‚îî‚îÄ LayerNorm
    ‚Üì
Linear Head (vocab_size)
    ‚Üì
–í—ã—Ö–æ–¥–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `vocab_size`: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)
- `d_model`: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (128-512)
- `num_layers`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–±–ª–æ–∫–æ–≤ (4-12)
- `num_heads`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (4-16)
- `d_ff`: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å feed-forward —Å–µ—Ç–∏ (–æ–±—ã—á–Ω–æ 4√ód_model)
- `max_seq_len`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### Multi-Head Attention
–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞:

```python
Q, K, V = Linear(x)  # Query, Key, Value
Attention = softmax(QK^T / ‚àöd_k) √ó V
```

–ö–∞–∂–¥–∞—è "–≥–æ–ª–æ–≤–∞" —É—á–∏—Ç —Å–≤–æ–π –∞—Å–ø–µ–∫—Ç –≤–Ω–∏–º–∞–Ω–∏—è (—Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫–∞, etc.)

---

### 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (`tokenizer.py`)

#### SimpleTokenizer
–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –æ–±—Ä–∞—Ç–Ω–æ:

**–ü—Ä–æ—Ü–µ—Å—Å:**
1. **–û–±—É—á–µ–Ω–∏–µ**: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–∞, –≤—ã–¥–µ–ª—è–µ—Ç —Ç–æ–ø-N —Ç–æ–∫–µ–Ω–æ–≤
2. **–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ**: –¢–µ–∫—Å—Ç ‚Üí –°–ø–∏—Å–æ–∫ ID —Ç–æ–∫–µ–Ω–æ–≤
3. **–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ**: ID ‚Üí –¢–µ–∫—Å—Ç

**–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:**
- `<PAD>` (0): –ü–∞–¥–¥–∏–Ω–≥ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã
- `<UNK>` (1): –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω
- `<BOS>` (2): –ù–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- `<EOS>` (3): –ö–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

---

### 3. AZR Trainer (`azr_trainer.py`)

#### –ú–µ—Ç–æ–¥ Absolute Zero Reasoner

**–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:**

```
1. SELF-PLAY –§–ê–ó–ê
   ‚îú‚îÄ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤
   ‚îú‚îÄ –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
   ‚îî‚îÄ –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –Ω–∞–≥—Ä–∞–¥–∞ (reward)

2. –û–ë–£–ß–ï–ù–ò–ï
   ‚îú‚îÄ –ë–∞—Ç—á –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   ‚îú‚îÄ Forward pass
   ‚îú‚îÄ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
   ‚îú‚îÄ Backward pass
   ‚îî‚îÄ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤

3. –ü–û–í–¢–û–† ‚Üí –ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
```

#### –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã (Reward)

–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç:

```python
reward = diversity_score √ó 0.5 + length_score √ó 0.5
```

- **Diversity**: –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è)
- **Length**: –î–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ–æ—â—Ä—è–µ—Ç –ø–æ–ª–Ω—ã–µ –º—ã—Å–ª–∏)

**–ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å:**
- –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å (—Å–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞)
- –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–º–ø—Ç—É
- –ù–æ–≤–∏–∑–Ω–∞ –∏–¥–µ–π

---

### 4. FastAPI Backend (`server.py`)

#### REST API Endpoints

| Endpoint | –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-------|----------|
| `/` | GET | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å |
| `/create_model` | POST | –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ |
| `/upload_book` | POST | –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |
| `/books` | GET | –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∫–Ω–∏–≥ |
| `/models` | GET | –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π |
| `/train` | POST | –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è (—Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å) |
| `/training_status` | GET | –°—Ç–∞—Ç—É—Å —Ç–µ–∫—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è |
| `/generate` | POST | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ |
| `/download_model/{name}` | GET | –°–∫–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å |

#### –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–µ—Ä:

```python
thread = threading.Thread(target=train_model_background)
thread.start()
```

–°—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ polling endpoint `/training_status`

---

## üîÑ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

### –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```
–ö–Ω–∏–≥–∞ (—Ç–µ–∫—Å—Ç)
    ‚Üì
–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ (overlapping)
    ‚Üì
–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    ‚Üì
–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä (input, target)
    ‚Üì
DataLoader (–±–∞—Ç—á–∏)
```

### –≠—Ç–∞–ø 2: –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª

```python
for epoch in range(max_epochs):
    for batch in dataloader:
        # Supervised Learning
        loss = model(batch.input, batch.target)
        loss.backward()
        optimizer.step()
        
        # Self-Play (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏)
        if iteration % 10 == 0:
            generated = model.generate(prompts)
            rewards = compute_reward(generated)
            # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
```

### –≠—Ç–∞–ø 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

- **Optimizer**: AdamW —Å weight decay
- **Scheduler**: Cosine Annealing (–ø–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ LR)
- **Gradient Clipping**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

```python
optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=max_iterations)
clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞

### Self-Attention

```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) √ó V

–≥–¥–µ:
Q = xW_Q  (queries)
K = xW_K  (keys)
V = xW_V  (values)
```

### Multi-Head Attention

```
MultiHead(x) = Concat(head_1, ..., head_h) √ó W_O

–≥–¥–µ head_i = Attention(xW_Q^i, xW_K^i, xW_V^i)
```

### Feed-Forward Network

```
FFN(x) = GELU(xW_1 + b_1) √ó W_2 + b_2
```

### Layer Normalization

```
LayerNorm(x) = Œ≥ √ó (x - Œº) / ‚àö(œÉ¬≤ + Œµ) + Œ≤
```

---

## üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

### –ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

```python
for _ in range(max_new_tokens):
    logits = model(context)
    next_token = sample(logits[-1])
    context = append(context, next_token)
```

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è

1. **Greedy**: –í—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
   ```python
   next_token = argmax(probs)
   ```

2. **Temperature Sampling**: –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
   ```python
   probs = softmax(logits / temperature)
   next_token = multinomial(probs)
   ```
   - T ‚Üí 0: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ
   - T = 1: –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
   - T ‚Üí ‚àû: —Å–ª—É—á–∞–π–Ω–æ

3. **Top-K Sampling**: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞
   ```python
   top_k_probs = probs[topk(probs, k)]
   next_token = multinomial(top_k_probs)
   ```

---

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### Loss (–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å)

```
CrossEntropyLoss = -Œ£ y_true √ó log(y_pred)
```

–ú–µ–Ω—å—à–µ ‚Üí –õ—É—á—à–µ (–º–æ–¥–µ–ª—å —Ç–æ—á–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç)

### Perplexity

```
Perplexity = exp(CrossEntropyLoss)
```

–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ "—É–¥–∏–≤–ª–µ–Ω–∏—è" –º–æ–¥–µ–ª–∏

### Reward (–ù–∞–≥—Ä–∞–¥–∞ –≤ self-play)

–ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –ë–æ–ª—å—à–µ ‚Üí –õ—É—á—à–µ

---

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ú–∞–ª–∞—è | –°—Ä–µ–¥–Ω—è—è | –ë–æ–ª—å—à–∞—è |
|----------|-------|---------|---------|
| d_model | 128 | 256-384 | 512-768 |
| num_layers | 4-6 | 8-10 | 12-16 |
| num_heads | 4 | 8 | 12-16 |
| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã** | ~5M | ~20M | ~100M+ |

### –û–±—É—á–µ–Ω–∏–µ

- **Learning Rate**: 1e-4 –¥–æ 5e-4 (–º–µ–Ω—å—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π)
- **Batch Size**: 8-32 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–∞–º—è—Ç–∏ GPU)
- **Max Iterations**: 10K (—Ç–µ—Å—Ç) –¥–æ 1M+ (–ø—Ä–æ–¥–∞–∫—à–Ω)
- **Save Every**: 500-1000 –∏—Ç–µ—Ä–∞—Ü–∏–π

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è

- **Temperature**: 0.5 (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ) –¥–æ 1.5 (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ)
- **Top-K**: 20-50 (–±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è)

---

## üí° –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ AZR –ø–æ–¥—Ö–æ–¥–∞

1. **–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ**: –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç—Å—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: Self-play –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞
3. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ç –º–∞–ª—ã—Ö –¥–æ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
4. **–ì–∏–±–∫–æ—Å—Ç—å**: –õ–µ–≥–∫–æ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–¥ –∑–∞–¥–∞—á—É

---

## üöÄ –ë—É–¥—É—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è

- [ ] Reinforcement Learning (PPO, A3C) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [ ] Beam Search –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- [ ] Nucleus (Top-P) sampling
- [ ] Multi-task learning (–ø–µ—Ä–µ–≤–æ–¥, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è)
- [ ] Model distillation (—Å–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏)
- [ ] Distributed training (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU)
- [ ] LoRA/QLoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning
- [ ] Evaluation suite (BLEU, ROUGE, BERTScore)

---

## üìö –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã

2. **Language Models are Unsupervised Multitask Learners** (Radford et al., 2019)
   - GPT-2, –æ—Å–Ω–æ–≤—ã –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è

3. **Training language models to follow instructions with human feedback** (Ouyang et al., 2022)
   - RLHF, InstructGPT

4. **Absolute Zero Reasoner** (2025)
   - Self-play –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

---

**–°–æ–∑–¥–∞–Ω–æ —Å ‚ù§Ô∏è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –∏ —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤ AI**
