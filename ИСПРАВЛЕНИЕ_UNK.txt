╔══════════════════════════════════════════════════════════════╗
║        КАК ИЗБАВИТЬСЯ ОТ <UNK> ТОКЕНОВ                     ║
║        Пошаговое решение проблемы                           ║
╚══════════════════════════════════════════════════════════════╝

🔴 ПРОБЛЕМА:
   Модель генерирует много <UNK> <UNK> <UNK> вместо слов


═══════════════════════════════════════════════════════════════
🔍 ПРИЧИНЫ
═══════════════════════════════════════════════════════════════

<UNK> = Unknown Token (неизвестный токен)

Появляется когда:
   1. Слово НЕТ в словаре токенизатора
   2. Токенизатор плохо обучен
   3. vocab_size слишком маленький
   4. Слишком мало итераций обучения


═══════════════════════════════════════════════════════════════
✅ ЧТО Я ИСПРАВИЛ
═══════════════════════════════════════════════════════════════

В файле server_with_datasets.py:

БЫЛО (плохо):
```python
# Обучался только на первых 100 чанках
all_text = " ".join(texts[:100])
tokenizer.train([all_text])
```

СТАЛО (хорошо):
```python
# Обучается на ВСЕХ прикреплённых датасетах
batch_size = 100
all_texts_combined = []
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    all_texts_combined.append(" ".join(batch))

tokenizer.train(all_texts_combined)
print(f"Tokenizer vocabulary size: {len(tokenizer.token_to_id)} tokens")
```

Теперь токенизатор:
   ✅ Видит ВСЕ тексты из датасетов
   ✅ Строит словарь из всех встреченных слов
   ✅ Знает больше слов


═══════════════════════════════════════════════════════════════
🎯 ЧТО ДЕЛАТЬ СЕЙЧАС
═══════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────┐
│ 1️⃣  УДАЛИТЬ СТАРЫЙ ТОКЕНИЗАТОР                              │
└─────────────────────────────────────────────────────────────┘

Старый токенизатор плохо обучен!

Как удалить:
   1. Откройте папку: C:\Users\clavi\Desktop\для ии\models\
   2. Найдите папку вашей модели (например "my_model")
   3. УДАЛИТЕ файл: tokenizer.pkl
   4. При следующем обучении создастся новый!


┌─────────────────────────────────────────────────────────────┐
│ 2️⃣  ПЕРЕОБУЧИТЬ МОДЕЛЬ                                      │
└─────────────────────────────────────────────────────────────┘

После удаления tokenizer.pkl:

1. Откройте веб-интерфейс
2. Вкладка "Обучение"
3. Выберите модель
4. Настройки:
   • Max Iterations: 10000 (минимум!)
   • Batch Size: 16-32
   • Learning Rate: 0.0003
   
5. Нажмите "Начать обучение"

В консоли увидите:
   Training tokenizer on all attached datasets...
   Total text chunks: 500
   Training tokenizer on 5 batches...
   Tokenizer vocabulary size: 7856 tokens
   Tokenizer saved!

Если видите 7856+ токенов → ✅ Хорошо!
Если видите ~100 токенов → ❌ Что-то не так


┌─────────────────────────────────────────────────────────────┐
│ 3️⃣  УВЕЛИЧИТЬ VOCAB_SIZE (опционально)                     │
└─────────────────────────────────────────────────────────────┘

Если <UNK> всё ещё много:

Создайте НОВУЮ модель с большим словарём:

Вкладка "Создать":
   Название: my_model_large
   Vocab Size: 15000 ← Вместо 8000!
   D Model: 256
   Num Layers: 6
   ...

Больше vocab_size = меньше <UNK> = лучше качество


┌─────────────────────────────────────────────────────────────┐
│ 4️⃣  ДОБАВИТЬ БОЛЬШЕ ДАННЫХ                                  │
└─────────────────────────────────────────────────────────────┘

Больше текста = больше слов в словаре = меньше <UNK>

Рекомендую:
   • Минимум: 1-2 книги (1-5 MB текста)
   • Оптимально: 5-10 книг (10-50 MB)
   • Идеально: 20+ книг разных жанров

Где взять:
   • Gutenberg.org (бесплатные книги)
   • Lib.rus.ec (русские книги)
   • Википедия дампы
   • Ваши собственные тексты


┌─────────────────────────────────────────────────────────────┐
│ 5️⃣  ОБУЧАТЬ ДОЛЬШЕ                                          │
└─────────────────────────────────────────────────────────────┘

1,000 итераций - это ОЧЕНЬ мало!

Рекомендации:
   ⚠️  1,000 = быстрый тест (плохое качество)
   ✅ 10,000 = минимум для видимых результатов
   ⭐ 50,000 = хорошее качество
   🏆 100,000+ = отличное качество

Почему важно:
   • Модель учится на каждой итерации
   • 1,000 итераций = видела примеры 1 раз
   • 100,000 итераций = видела примеры ~100 раз
   • Повторение = запоминание паттернов!


═══════════════════════════════════════════════════════════════
🔧 ДИАГНОСТИКА
═══════════════════════════════════════════════════════════════

Проверьте сколько слов знает токенизатор:

1. Откройте Python консоль
2. Запустите:

```python
from tokenizer import SimpleTokenizer

# Загрузите токенизатор
tokenizer = SimpleTokenizer.load("models/my_model/tokenizer.pkl")

# Проверьте размер словаря
print(f"Словарь: {len(tokenizer.token_to_id)} слов")

# Проверьте конкретные слова
test_words = ["нейросеть", "искусственный", "интеллект", "программа"]
for word in test_words:
    token_id = tokenizer.token_to_id.get(word.lower(), None)
    if token_id:
        print(f"✅ '{word}' -> ID {token_id}")
    else:
        print(f"❌ '{word}' -> <UNK>")

# Проверьте предложение
text = "Нейросеть это программа"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)
print(f"\nИсходный: {text}")
print(f"Декодированный: {decoded}")
```

Ожидаемый результат:
   ✅ Словарь: 7500-8000 слов
   ✅ Все тестовые слова найдены
   ✅ Декодированный текст читаемый


Если видите:
   ❌ Словарь: 100-500 слов → Переобучите!
   ❌ Много <UNK> в тесте → Нужно больше vocab_size
   ❌ Декодированный текст нечитаемый → Больше итераций!


═══════════════════════════════════════════════════════════════
📋 ЧЕКЛИСТ ДЕЙСТВИЙ
═══════════════════════════════════════════════════════════════

Выполните по порядку:

□ 1. Удалить старый tokenizer.pkl
     Путь: models/<your_model>/tokenizer.pkl

□ 2. Убедиться что датасеты прикреплены
     Вкладка "Датасеты" → Проверить прикреплённые

□ 3. Начать обучение с исправленным кодом
     Минимум 10,000 итераций!

□ 4. Дождаться завершения
     Следить за графиком: Loss должен падать

□ 5. Протестировать генерацию
     Вкладка "Генерация" → Проверить результат

□ 6. Если <UNK> всё ещё много:
     → Создать новую модель с vocab_size=15000
     → Добавить больше датасетов
     → Обучить на 50,000+ итераций


═══════════════════════════════════════════════════════════════
🎓 ПОНИМАНИЕ ПРОБЛЕМЫ
═══════════════════════════════════════════════════════════════

Почему появляются <UNK>?

Пример:

Токенизатор обучен на тексте:
   "Кот сидел на коврике. Собака лаяла."

Словарь: [кот, сидел, на, коврике, собака, лаяла, ., и т.д.]

Вы пытаетесь закодировать:
   "Котёнок играл"

Результат:
   "Котёнок" → <UNK> (не в словаре!)
   "играл" → <UNK> (не в словаре!)

Решение:
   • Обучить на большем количестве текстов
   • Увеличить vocab_size
   • Использовать больше данных


═══════════════════════════════════════════════════════════════
⚡ БЫСТРОЕ РЕШЕНИЕ (5 минут)
═══════════════════════════════════════════════════════════════

Если нужен быстрый результат:

1. Удалите tokenizer.pkl
2. Запустите обучение на 1000 итераций (проверка)
3. Проверьте генерацию
4. Если лучше → продолжите на 10,000
5. Если не лучше → смотрите "Расширенная диагностика"


═══════════════════════════════════════════════════════════════
🔬 РАСШИРЕННАЯ ДИАГНОСТИКА
═══════════════════════════════════════════════════════════════

Если <UNK> НЕ исчезают:

□ Проверка 1: Логи обучения
   Ищите строку:
   "Tokenizer vocabulary size: XXXX tokens"
   
   Если XXXX < 5000 → мало данных
   Если XXXX = 8000 → хорошо!

□ Проверка 2: Размер датасетов
   Вкладка "Датасеты" → Посмотреть размеры
   
   Если < 1 MB → мало!
   Если > 5 MB → отлично!

□ Проверка 3: Количество итераций
   После 1,000 → плохо
   После 10,000 → лучше
   После 100,000 → хорошо

□ Проверка 4: Loss на графике
   Должен падать!
   Если не падает → проблема с обучением


═══════════════════════════════════════════════════════════════
💡 СОВЕТЫ
═══════════════════════════════════════════════════════════════

1. Терпение!
   Обучение занимает время
   10,000 итераций ≈ 1-2 часа на CPU

2. Качество данных важнее количества
   Лучше 3 хорошие книги, чем 100 статей

3. Смотрите на Loss
   Если падает → всё работает!
   Если стоит → что-то не так

4. Экспериментируйте
   Попробуйте разные vocab_size
   Попробуйте разные learning_rate

5. Сохраняйте промежуточные результаты
   Каждые 1000 итераций автоматически сохраняется checkpoint


═══════════════════════════════════════════════════════════════
✨ ОЖИДАЕМЫЙ РЕЗУЛЬТАТ
═══════════════════════════════════════════════════════════════

После исправлений:

БЫЛО:
   Промпт: "Что такое нейросеть?"
   Ответ: "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK>"

СТАНЕТ (после 10,000 итераций):
   Промпт: "Что такое нейросеть?"
   Ответ: "Нейросеть это программа которая обучается на данных
          и может распознавать образы и генерировать текст"

Не идеально, но БЕЗ <UNK>!

После 100,000 итераций будет ещё лучше!


═══════════════════════════════════════════════════════════════
❓ FAQ
═══════════════════════════════════════════════════════════════

Q: Сколько времени займёт переобучение?
A: 10,000 итераций ≈ 1-2 часа (CPU) или 10-20 минут (GPU)

Q: Нужно ли удалять модель полностью?
A: НЕТ! Только tokenizer.pkl. Модель можно дообучить.

Q: Что если я добавлю датасет ПОСЛЕ обучения?
A: Нужно переобучить токенизатор. Удалите tokenizer.pkl
   и запустите обучение снова.

Q: Можно ли использовать готовый токенизатор?
A: Можно, но лучше обучить на ваших данных.

Q: Vocab_size больше = всегда лучше?
A: Не всегда. Больше vocab = больше памяти.
   8000-15000 оптимально для большинства задач.


═══════════════════════════════════════════════════════════════
🎯 ИТОГ
═══════════════════════════════════════════════════════════════

Проблема РЕШЕНА в коде!

Ваши действия:
   1. Удалите tokenizer.pkl
   2. Переобучите минимум на 10,000 итераций
   3. Проверьте результат
   4. Если нужно - увеличьте vocab_size и обучайте дольше

Удачи! 🚀
