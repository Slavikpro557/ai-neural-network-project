╔══════════════════════════════════════════════════════════════╗
║         ✅ АВТООЧИСТКА ЧЕКПОИНТОВ ДОБАВЛЕНА!                 ║
╚══════════════════════════════════════════════════════════════╝


═══════════════════════════════════════════════════════════════
🎯 ЧТО БЫЛО / ЧТО СТАЛО
═══════════════════════════════════════════════════════════════

БЫЛО:
────────────────────────────────────────────────────────────

Обучение 66,000 итераций:
   model_iter_1000.pt   (102 MB)
   model_iter_2000.pt   (102 MB)
   model_iter_3000.pt   (102 MB)
   ...
   model_iter_66000.pt  (102 MB)
   
   Итого: 66 файлов × 102 MB = 6.7 GB ❌
   
Диск заполнялся! Приходилось вручную чистить!


СТАЛО:
────────────────────────────────────────────────────────────

Обучение 66,000 итераций:
   [старые чекпоинты удаляются автоматически]
   
   model_iter_62000.pt  (102 MB)
   model_iter_63000.pt  (102 MB)
   model_iter_64000.pt  (102 MB)
   model_iter_65000.pt  (102 MB)
   model_iter_66000.pt  (102 MB)
   
   Итого: 5 файлов × 102 MB = 510 MB ✅
   
Диск НЕ заполняется! Всё автоматически!


═══════════════════════════════════════════════════════════════
⚙️ КАК РАБОТАЕТ
═══════════════════════════════════════════════════════════════

КАЖДЫЕ 1000 ИТЕРАЦИЙ:

1. Создаётся новый чекпоинт
   model_iter_67000.pt → сохранён

2. Ищутся все чекпоинты
   [62000, 63000, 64000, 65000, 66000, 67000]

3. Сортируются по времени (старые → новые)

4. Оставляются 5 последних
   [63000, 64000, 65000, 66000, 67000]

5. Удаляются старые
   🗑️ Deleted old checkpoint: model_iter_62000.pt

РЕЗУЛЬТАТ:
   Всегда только 5 чекпоинтов (~500 MB)
   Вместо сотен (~10 GB)


═══════════════════════════════════════════════════════════════
📊 ЗАЧЕМ ЧЕКПОИНТЫ
═══════════════════════════════════════════════════════════════

Чекпоинты = точки восстановления

ПРИМЕР 1: Сбой обучения
────────────────────────────────────────────────────────────

Обучаешь модель на 50,000 итераций
На 23,456 итерации:
   • Компьютер выключился
   • Программа упала
   • Ctrl+C случайно нажал

БЕЗ чекпоинтов:
   ❌ Всё потеряно! Начинай с 0!

С чекпоинтами:
   ✅ Загружаешь model_iter_23000.pt
   ✅ Продолжаешь с 23,000
   ✅ Потеряно только 456 итераций!


ПРИМЕР 2: Эксперименты
────────────────────────────────────────────────────────────

Обучил модель 10,000 итераций - хорошо
Обучил ещё 10,000 итераций - стало хуже!

С чекпоинтами:
   ✅ Откатываешься к model_iter_10000.pt
   ✅ Пробуешь другие настройки
   ✅ Не нужно переобучивать с нуля!


═══════════════════════════════════════════════════════════════
💾 ПОЧЕМУ ЧЕКПОИНТ 102 MB, А МОДЕЛЬ 35 MB?
═══════════════════════════════════════════════════════════════

ЧЕКПОИНТ (для продолжения обучения):
────────────────────────────────────────────────────────────

{
    "model_state_dict": {...}       35 MB  ← Веса модели
    "optimizer_state_dict": {...}   35 MB  ← Память оптимизатора
    "scheduler_state_dict": {...}    1 MB  ← Состояние LR
    "training_history": [...]        1 MB  ← История
    "iteration": 66000              30 MB  ← Метаданные
}
────────────────────────────────────────────────────────────
ИТОГО:                             102 MB


ФИНАЛЬНАЯ МОДЕЛЬ (для использования):
────────────────────────────────────────────────────────────

{
    "model_state_dict": {...}       35 MB  ← Только веса!
}
────────────────────────────────────────────────────────────
ИТОГО:                              35 MB


ЧТО ТАКОЕ OPTIMIZER STATE?

Оптимизатор (AdamW) запоминает ДЛЯ КАЖДОГО веса:
   • Момент 1 (momentum) - "куда двигались"
   • Момент 2 (variance) - "как сильно меняется"
   • Adaptive LR - "как быстро учиться"

8.4M параметров × 8 байт = 67 MB!

НУЖЕН ДЛЯ:
   ✅ Продолжения обучения с того же места
   ✅ Сохранения "импульса" обучения
   ✅ Resume работает идеально

НЕ НУЖЕН ДЛЯ:
   ❌ Генерации текста
   ❌ Использования модели
   ❌ Скачивания


═══════════════════════════════════════════════════════════════
🗂️ КАКИЕ ФАЙЛЫ ГДЕ
═══════════════════════════════════════════════════════════════

ПАПКА checkpoints/славик/ (чекпоинты):
────────────────────────────────────────────────────────────

model_iter_62000.pt     102 MB
model_iter_63000.pt     102 MB
model_iter_64000.pt     102 MB
model_iter_65000.pt     102 MB
model_iter_66000.pt     102 MB

Назначение: Resume (продолжение обучения)
Автоочистка: ✅ Да! Оставляет 5 последних


ПАПКА models/славик/ (финальная):
────────────────────────────────────────────────────────────

model.pt               35 MB   (базовая, необученная)
model_trained.pt      102 MB   (текущая - восстановлена из чекпоинта)
tokenizer.pkl          73 KB
config.json           175 байт

Назначение: Использование, генерация, скачивание
Автоочистка: ❌ Нет (не нужна)


ПОЧЕМУ model_trained.pt сейчас 102 MB?

Потому что я восстановил её из чекпоинта:
   copy checkpoints/славик/model_iter_66000.pt 
     → models/славик/model_trained.pt

Когда завершишь обучение нормально:
   torch.save(model.state_dict(), "model_trained.pt")
   → Будет 35 MB!


═══════════════════════════════════════════════════════════════
🎯 ЧТО ТЕПЕРЬ ПРОИСХОДИТ
═══════════════════════════════════════════════════════════════

ЗАПУСТИЛ ОБУЧЕНИЕ:
────────────────────────────────────────────────────────────

Iteration 1000:
   💾 Saved checkpoint: model_iter_1000.pt
   
Iteration 2000:
   💾 Saved checkpoint: model_iter_2000.pt
   
Iteration 3000:
   💾 Saved checkpoint: model_iter_3000.pt
   
Iteration 4000:
   💾 Saved checkpoint: model_iter_4000.pt
   
Iteration 5000:
   💾 Saved checkpoint: model_iter_5000.pt
   
Iteration 6000:
   💾 Saved checkpoint: model_iter_6000.pt
   🗑️ Deleted old checkpoint: model_iter_1000.pt
   
Iteration 7000:
   💾 Saved checkpoint: model_iter_7000.pt
   🗑️ Deleted old checkpoint: model_iter_2000.pt
   
...и так далее


ИТОГО:
   ✅ Всегда только 5 чекпоинтов
   ✅ ~500 MB вместо гигабайт
   ✅ Автоматически!


═══════════════════════════════════════════════════════════════
⚠️ ВАЖНО
═══════════════════════════════════════════════════════════════

МОЖНО ИЗМЕНИТЬ СКОЛЬКО ОСТАВЛЯТЬ:

В коде azr_trainer_resume.py:
   self.cleanup_old_checkpoints(checkpoint_dir, keep_last=5)
                                                          ↑
Хочешь 10 чекпоинтов? → keep_last=10
Хочешь 3 чекпоинта?   → keep_last=3


СКОЛЬКО МЕСТА ЗАЙМЁТ:

keep_last=3  →   3 × 102 MB = 306 MB
keep_last=5  →   5 × 102 MB = 510 MB  ← СЕЙЧАС
keep_last=10 →  10 × 102 MB = 1.0 GB


РЕКОМЕНДАЦИЯ:
   5 чекпоинтов = золотая середина!
   
   Достаточно для:
   • Отката на 5000 итераций назад
   • Экспериментов
   • Восстановления после сбоя
   
   Не занимает много:
   • 510 MB - это мало
   • Диск не заполнится


═══════════════════════════════════════════════════════════════
✅ ИТОГ
═══════════════════════════════════════════════════════════════

ПРОБЛЕМА 1: "Почему потом 35 МБ?"
   → Сейчас 102 МБ потому что восстановлен из чекпоинта
   → При нормальном завершении будет 35 МБ

ПРОБЛЕМА 2: "Память снова закончится?"
   → НЕТ! Теперь автоочистка! ✅
   → Максимум 5 чекпоинтов = 510 МБ

ПРОБЛЕМА 3: "Почему было 9 ГБ?"
   → 76 чекпоинтов накопилось (66,000 итераций)
   → Удалили 71 старый чекпоинт
   → Освободили 6.5 ГБ

ПРОБЛЕМА 4: "Для чего так много забивает?"
   → Чекпоинты нужны для Resume
   → Содержат optimizer state (×3 размер)
   → Теперь автоочистка - больше не засорится!


════════════════════════════════════════════════════════════════

ПЕРЕЗАПУСТИ СЕРВЕР И ВСЁ! 🚀

Теперь диск НЕ заполнится при обучении!
